# -*- coding: utf-8 -*-
"""LeafNet-CNN.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rPnjtOhBnwYoKsW58N0LSsHdg8AJzTPQ
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!mkdir -p /kaggle/working/mangoleaf

!kaggle datasets download -d tahmidmir/mangoleaf -p /kaggle/working/mangoleaf --unzip

!ls -R /kaggle/working/mangoleaf | head -n 120

import os, random
import numpy as np
import tensorflow as tf
from tensorflow.keras import layers, models, optimizers, callbacks
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc
import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd
from sklearn.preprocessing import label_binarize

seed = 42
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
os.environ['PYTHONHASHSEED'] = str(seed)

# Collects image file paths and corresponding class labels from the training dataset directory
DATA_ROOT = "/kaggle/working/mangoleaf/MangoLeafBD/Train"


paths, labels, class_names = gather_image_paths(DATA_ROOT)
print("Classes found:", class_names)
print("Total images:", len(paths))

def gather_image_paths(root_dir):
    paths, labels = [], []
    classes = []

    for d in sorted(os.listdir(root_dir)):
        dir_path = os.path.join(root_dir, d)
        if not os.path.isdir(dir_path):
            continue


        image_files = [f for f in os.listdir(dir_path) if f.lower().endswith(('.png','.jpg','.jpeg','.bmp'))]
        if len(image_files) == 0:
            continue

        classes.append(d)
        for f in image_files:
            paths.append(os.path.join(dir_path, f))
            labels.append(len(classes)-1)  # assign numeric label

    return np.array(paths), np.array(labels), classes


paths, labels, class_names = gather_image_paths(DATA_ROOT)
print("Classes found:", class_names)
print("Total images:", len(paths))

# Defines a custom CNN model (LeafNet) architecture for classifying mango leaf images into multiple disease categories
def build_leafnet(input_shape=(128,128,3), num_classes=8):
    inp = layers.Input(shape=input_shape)
    x = layers.Conv2D(32, (11,11), strides=(3,3), activation='relu', padding='same')(inp)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Conv2D(64, (11,11), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Conv2D(96, (5,5), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)

    x = layers.Conv2D(64, (5,5), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Conv2D(90, (3,3), activation='relu', padding='same')(x)
    x = layers.BatchNormalization()(x)
    x = layers.MaxPooling2D((2,2))(x)

    x = layers.Flatten()(x)
    x = layers.Dense(800, activation='relu')(x)
    x = layers.Dropout(0.6)(x)
    out = layers.Dense(num_classes, activation='softmax')(x)
    return models.Model(inp, out)

# Sets key training hyperparameters including batch size, learning rate, epochs, early stopping patience, and input image size
BATCH_SIZE = 10
LEARNING_RATE = 0.001
MAX_EPOCHS = 40
PATIENCE = 60
IMAGE_SIZE = (227,227)

# Performs 3-fold stratified cross-validation to train, validate, and test the LeafNet model using image data generators with augmentation and logs fold-wise performance
skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=seed)
fold_scores, all_y_true, all_y_pred = [], [], []

for fold_no, (train_index, test_index) in enumerate(skf.split(paths, labels), 1):
    print(f"\n=== Fold {fold_no} ===")

    train_paths, test_paths = paths[train_index], paths[test_index]
    train_labels, test_labels = labels[train_index], labels[test_index]


    idx = np.arange(len(train_paths))
    np.random.shuffle(idx)
    val_split = int(0.85 * len(idx))
    train_idx2, val_idx2 = idx[:val_split], idx[val_split:]

    train_paths_fold, train_labels_fold = train_paths[train_idx2], train_labels[train_idx2]
    val_paths_fold, val_labels_fold = train_paths[val_idx2], train_labels[val_idx2]


    train_datagen = ImageDataGenerator(
        rescale=1./255,
        rotation_range=10,
        horizontal_flip=True,
        brightness_range=(0.8, 1.2)
    )
    val_datagen = ImageDataGenerator(rescale=1./255)
    test_datagen = ImageDataGenerator(rescale=1./255)

    def make_gen(paths_arr, labels_arr, datagen, shuffle=True):
        df = pd.DataFrame({"filename": paths_arr, "class": labels_arr})
        df["class_str"] = df["class"].map(lambda x: class_names[int(x)])
        return datagen.flow_from_dataframe(
            df, x_col="filename", y_col="class_str",
            target_size=(160, 160),
            class_mode="categorical",
            batch_size=16,
            shuffle=shuffle)

    train_gen = make_gen(train_paths_fold, train_labels_fold, train_datagen)
    val_gen = make_gen(val_paths_fold, val_labels_fold, val_datagen, shuffle=False)
    test_gen = make_gen(test_paths, test_labels, test_datagen, shuffle=False)


    model = build_leafnet(input_shape=(160,160,3), num_classes=len(class_names))
    opt = optimizers.SGD(learning_rate=0.005, momentum=0.7, nesterov=False)
    model.compile(optimizer=opt, loss="categorical_crossentropy", metrics=["accuracy"])

    cb = [
        callbacks.EarlyStopping(monitor="val_loss", patience=10, restore_best_weights=True),
    ]

    history = model.fit(train_gen, validation_data=val_gen,
                        epochs=25, callbacks=cb, verbose=1)

    loss, acc = model.evaluate(test_gen, verbose=1)
    print(f"Fold {fold_no} Test Accuracy: {acc*100:.2f}%")
    fold_scores.append(acc)

    y_prob = model.predict(test_gen, verbose=1)
    y_pred = np.argmax(y_prob, axis=1)
    y_true = test_gen.classes[:len(y_pred)]

    all_y_true.append(y_true)
    all_y_pred.append(y_pred)

# Computes and visualizes overall model performance metrics and confusion matrix
from sklearn.metrics import (
    confusion_matrix, classification_report, precision_score,
    recall_score, f1_score, cohen_kappa_score
)

print("\nFold Accuracies:", [f"{x*100:.2f}%" for x in fold_scores])
mean_acc = np.mean(fold_scores)
print(f"Mean Accuracy: {mean_acc*100:.2f}%")
macro_precision = precision_score(y_true_all, y_pred_all, average='macro')
macro_recall = recall_score(y_true_all, y_pred_all, average='macro')
macro_f1 = f1_score(y_true_all, y_pred_all, average='macro')
print(f"Macro Precision: {macro_precision*100:.2f}%")
print(f"Macro Recall:    {macro_recall*100:.2f}%")
print(f"Macro F1-Score:  {macro_f1*100:.2f}%")

y_true_all = np.concatenate(all_y_true)
y_pred_all = np.concatenate(all_y_pred)

cm = confusion_matrix(y_true_all, y_pred_all)
plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted"); plt.ylabel("True")
plt.title("Confusion Matrix (All Folds)")
plt.show()

print("\nClassification Report:")
print(classification_report(y_true_all, y_pred_all, target_names=class_names))