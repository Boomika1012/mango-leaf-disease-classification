# -*- coding: utf-8 -*-
"""Ensemble CNN GoogLeNet + VGG16 style.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1JV5I1zSeC0P_FEsE9ntueX-a1_T-_QAC
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle

!cp kaggle.json ~/.kaggle/
!chmod 600 ~/.kaggle/kaggle.json
!mkdir -p /kaggle/working/mangoleaf

!kaggle datasets download -d tahmidmir/mangoleaf -p /kaggle/working/mangoleaf --unzip

!ls -R /kaggle/working/mangoleaf | head -n 120

import os
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, cohen_kappa_score, roc_auc_score
from tensorflow.keras import layers, Model, optimizers
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
from tensorflow.keras.applications import InceptionV3, VGG16
from tensorflow.keras.utils import to_categorical

DATASET_PATH = "/kaggle/working/mangoleaf/MangoLeafBD"
TRAIN_DIR = os.path.join(DATASET_PATH, "Train")
TEST_DIR  = os.path.join(DATASET_PATH, "Test")

IMG_SIZE = 224
BATCH_SIZE = 32
EPOCHS = 30
MODEL_FILE = "ensemble_best.keras"

# Load, augment, and prepare training, validation, and test image datasets for classification
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=30,
    width_shift_range=0.15,
    height_shift_range=0.15,
    shear_range=0.15,
    zoom_range=0.15,
    horizontal_flip=True,
    brightness_range=[0.8, 1.2],
    fill_mode="nearest",
    validation_split=0.2
)

test_datagen = ImageDataGenerator(rescale=1./255)

train_gen = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='training',
    shuffle=True
)

val_gen = train_datagen.flow_from_directory(
    TRAIN_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    subset='validation',
    shuffle=True
)

test_gen = test_datagen.flow_from_directory(
    TEST_DIR,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='categorical',
    shuffle=False
)

class_names = list(train_gen.class_indices.keys())
num_classes = len(class_names)

# Build an ensemble CNN model by combining features from InceptionV3 and VGG16 for image classification
input_tensor = layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))

base1 = InceptionV3(weights="imagenet", include_top=False, input_tensor=input_tensor)
base2 = VGG16(weights="imagenet", include_top=False, input_tensor=input_tensor)

base1.trainable = False
base2.trainable = False

x1 = layers.GlobalAveragePooling2D()(base1.output)
x1 = layers.Dense(512, activation='relu')(x1)
x1 = layers.Dropout(0.4)(x1)

x2 = layers.GlobalAveragePooling2D()(base2.output)
x2 = layers.Dense(512, activation='relu')(x2)
x2 = layers.Dropout(0.4)(x2)

merged = layers.Concatenate()([x1, x2])
merged = layers.Dense(256, activation='relu')(merged)
merged = layers.Dropout(0.3)(merged)

outputs = layers.Dense(num_classes, activation='softmax')(merged)

ensemble_model = Model(inputs=input_tensor, outputs=outputs)

# Compile the ensemble model with Adam optimizer, set loss and accuracy metrics, training callbacks for best performance
ensemble_model.compile(
    optimizer=optimizers.Adam(1e-4),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

callbacks = [
    EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True, verbose=1),
    ModelCheckpoint(MODEL_FILE, save_best_only=True, monitor='val_loss', verbose=1),
    ReduceLROnPlateau(monitor='val_loss', factor=0.25, patience=3, min_lr=1e-6, verbose=1)
]

# Train the ensemble model using training and validation data with defined epochs and callbacks
history = ensemble_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=EPOCHS,
    callbacks=callbacks
)

# Fine-tune the ensemble model by unfreezing base layers and retraining with a lower learning rate
base1.trainable = True
base2.trainable = True

ensemble_model.compile(
    optimizer=optimizers.Adam(1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

fine_tune_history = ensemble_model.fit(
    train_gen,
    validation_data=val_gen,
    epochs=10,
    callbacks=callbacks
)

# Evaluate model performance on test data using multiple classification metrics and print the results
y_true = test_gen.classes
y_prob = ensemble_model.predict(test_gen, verbose=1)
y_pred = np.argmax(y_prob, axis=1)

cm = confusion_matrix(y_true, y_pred)
acc = accuracy_score(y_true, y_pred)
prec = precision_score(y_true, y_pred, average='macro', zero_division=0)
rec = recall_score(y_true, y_pred, average='macro', zero_division=0)
f1s = f1_score(y_true, y_pred, average='macro', zero_division=0)
sensitivity = np.mean(np.diag(cm) / np.sum(cm, axis=1))
specificity = np.mean([cm[i,i]/(np.sum(cm)-np.sum(cm[i,:])) for i in range(num_classes)])
cohen_kappa = cohen_kappa_score(y_true, y_pred)

y_true_onehot = to_categorical(y_true, num_classes=num_classes)
try:
    roc_auc = roc_auc_score(y_true_onehot, y_prob, multi_class='ovr')
except:
    roc_auc = None

print(f"\nAccuracy: {acc:.4f}")
print(f"Precision: {prec:.4f}")
print(f"Recall: {rec:.4f}")
print(f"Sensitivity: {sensitivity:.4f}")
print(f"Specificity: {specificity:.4f}")
print(f"F1 Score: {f1s:.4f}")
print(f"Cohen Kappa: {cohen_kappa:.4f}")
if roc_auc:
    print(f"ROC-AUC: {roc_auc:.4f}")

# Plot training/validation accuracy and loss, visualize confusion matrix
import seaborn as sns

plt.figure()
plt.plot(history.history['accuracy'] + fine_tune_history.history['accuracy'], label='train_acc')
plt.plot(history.history['val_accuracy'] + fine_tune_history.history['val_accuracy'], label='val_acc')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

plt.figure()
plt.plot(history.history['loss'] + fine_tune_history.history['loss'], label='train_loss')
plt.plot(history.history['val_loss'] + fine_tune_history.history['val_loss'], label='val_loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.show()

plt.figure(figsize=(10,8))
sns.heatmap(cm, annot=True, fmt='d', cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel('Predicted')
plt.ylabel('True')
plt.title('Confusion Matrix')
plt.show()

print(f"\nSaved ensemble best model to: {os.path.abspath(MODEL_FILE)}")